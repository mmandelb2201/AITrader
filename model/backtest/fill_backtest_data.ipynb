{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de00c449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd5ee75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Assets to fetch: 15\n",
      "   BTC, AAVE, SOL, STRK, ETH, OP, UNI, LINK, MKR, ARB, AVAX, BNB, NEAR, ADA, POL\n"
     ]
    }
   ],
   "source": [
    "# Assets needed (from eth_xgboost_golden_trio)\n",
    "assets = [\n",
    "    \"BTC\", \"AAVE\", \"SOL\", \"STRK\", \"ETH\", \"OP\", \"UNI\", \"LINK\", \"MKR\",\n",
    "    \"ARB\", \"AVAX\", \"BNB\", \"NEAR\", \"ADA\", \"POL\"\n",
    "]\n",
    "\n",
    "# Symbol mapping (from specific_assets_data_grabber)\n",
    "ASSET_SYMBOLS = {\n",
    "    'BTC': 'COINBASE_SPOT_BTC_USD',\n",
    "    'ETH': 'COINBASE_SPOT_ETH_USD', \n",
    "    'SOL': 'COINBASE_SPOT_SOL_USD',\n",
    "    'ADA': 'COINBASE_SPOT_ADA_USD',\n",
    "    'AVAX': 'COINBASE_SPOT_AVAX_USD',\n",
    "    'LINK': 'COINBASE_SPOT_LINK_USD',\n",
    "    'UNI': 'COINBASE_SPOT_UNI_USD',\n",
    "    'AAVE': 'COINBASE_SPOT_AAVE_USD',\n",
    "    'ARB': 'COINBASE_SPOT_ARB_USD',\n",
    "    'MKR': 'COINBASE_SPOT_MKR_USD',\n",
    "    'NEAR': 'KRAKEN_SPOT_NEAR_USD',\n",
    "    'BNB': 'KRAKEN_SPOT_BNB_USD',\n",
    "    'STRK': 'KRAKEN_SPOT_STRK_USD',\n",
    "    'POL': 'KRAKEN_SPOT_POL_USD',\n",
    "    'OP': 'BITSTAMP_SPOT_OP_USD'\n",
    "}\n",
    "\n",
    "print(f\"üìä Assets to fetch: {len(assets)}\")\n",
    "print(f\"   {', '.join(assets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3012b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîë API Key loaded: ‚úÖ Yes\n",
      "‚è±Ô∏è  Granularity: 5 seconds\n"
     ]
    }
   ],
   "source": [
    "# ---- Config ----\n",
    "base_path = \"./data\"\n",
    "ref_file = os.path.join(base_path, \"ETHUSD_5s.csv\")\n",
    "output_file = os.path.join(base_path, \"combined_assets_5s.csv\")\n",
    "\n",
    "COINAPI_KEY = os.getenv('COIN_API_KEY')\n",
    "GRANULARITY_5S = 5  # 5 seconds\n",
    "MAX_RETRIES = 3\n",
    "NO_DATA_THRESHOLD = 3\n",
    "\n",
    "headers = {\n",
    "    'Accept': 'application/json',\n",
    "    'X-CoinAPI-Key': COINAPI_KEY\n",
    "}\n",
    "\n",
    "print(f\"üîë API Key loaded: {'‚úÖ Yes' if COINAPI_KEY else '‚ùå No'}\")\n",
    "print(f\"‚è±Ô∏è  Granularity: {GRANULARITY_5S} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1660c384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading reference timeline from ETHUSD_5s.csv...\n",
      "‚úÖ Reference timeline loaded\n",
      "   Time range: 2025-12-21 18:16:07.574055+00:00 to 2026-02-07 02:47:11.595901+00:00\n",
      "   Total timestamps: 767,250\n",
      "   Duration: 47 days\n"
     ]
    }
   ],
   "source": [
    "# ---- Load reference times from ETHUSD_5s.csv ----\n",
    "print(\"üìÇ Loading reference timeline from ETHUSD_5s.csv...\")\n",
    "ref = pd.read_csv(ref_file)\n",
    "\n",
    "# Find timestamp column\n",
    "time_col = None\n",
    "for possible_col in ['timestamp', 'time', 'datetime', 'Date']:\n",
    "    if possible_col in ref.columns:\n",
    "        time_col = possible_col\n",
    "        break\n",
    "\n",
    "if not time_col:\n",
    "    raise ValueError(f\"Could not find timestamp column in {ref_file}. Columns: {ref.columns.tolist()}\")\n",
    "\n",
    "ref[time_col] = pd.to_datetime(ref[time_col], utc=True)\n",
    "ref = ref.rename(columns={time_col: 'timestamp'})\n",
    "ref = ref[['timestamp']].sort_values('timestamp').drop_duplicates()\n",
    "\n",
    "print(f\"‚úÖ Reference timeline loaded\")\n",
    "print(f\"   Time range: {ref['timestamp'].min()} to {ref['timestamp'].max()}\")\n",
    "print(f\"   Total timestamps: {len(ref):,}\")\n",
    "print(f\"   Duration: {(ref['timestamp'].max() - ref['timestamp'].min()).days} days\")\n",
    "\n",
    "# Get time range for queries\n",
    "start_time = ref['timestamp'].min().to_pydatetime().replace(tzinfo=None)\n",
    "end_time = ref['timestamp'].max().to_pydatetime().replace(tzinfo=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66fc3b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Helper: Fetch ETH Funding Rates ----\n",
    "def fetch_eth_funding_rates(start_time, end_time):\n",
    "    \"\"\"\n",
    "    Fetch ETH perpetual futures funding rates.\n",
    "    Note: Funding rates are typically available for perpetual futures markets.\n",
    "    \"\"\"\n",
    "    # Try common perpetual symbols\n",
    "    perp_symbols = [\n",
    "        'BINANCE_PERP_ETH_USDT',\n",
    "        'DERIBIT_FUT_ETH_PERPETUAL',\n",
    "        'BYBIT_PERP_ETH_USDT'\n",
    "    ]\n",
    "    \n",
    "    for symbol in perp_symbols:\n",
    "        try:\n",
    "            print(f\"üîÑ Trying funding rates for {symbol}...\")\n",
    "            \n",
    "            # CoinAPI doesn't have a dedicated funding rates endpoint in free tier\n",
    "            # Alternative: Use quotes or metrics endpoint if available\n",
    "            # For now, we'll return None and document this limitation\n",
    "            print(f\"   ‚ö†Ô∏è  Funding rates require specialized CoinAPI subscription\")\n",
    "            print(f\"   ‚ÑπÔ∏è  Consider using exchange-specific APIs (Binance, Bybit, etc.)\")\n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Failed for {symbol}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3953d6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Helper: Fetch ETH Trades Data ----\n",
    "def fetch_eth_trades(start_time, end_time, symbol_id='COINBASE_SPOT_ETH_USD'):\n",
    "    \"\"\"\n",
    "    Fetch ETH trade data from CoinAPI and aggregate to 5s intervals.\n",
    "    Note: CoinAPI trades endpoint requires date-by-date queries for historical data.\n",
    "    \"\"\"\n",
    "    url = f\"https://rest.coinapi.io/v1/trades/{symbol_id}/history\"\n",
    "    \n",
    "    all_trades = []\n",
    "    current_date = start_time.date()\n",
    "    end_date = end_time.date()\n",
    "    \n",
    "    print(f\"üîÑ Fetching ETH trades from {current_date} to {end_date}...\")\n",
    "    \n",
    "    while current_date <= end_date:\n",
    "        params = {\n",
    "            'time_start': datetime.combine(current_date, datetime.min.time()).isoformat() + 'Z',\n",
    "            'time_end': datetime.combine(current_date, datetime.max.time()).isoformat() + 'Z',\n",
    "            'limit': 100000\n",
    "        }\n",
    "        \n",
    "        for attempt in range(MAX_RETRIES):\n",
    "            try:\n",
    "                r = requests.get(url, headers=headers, params=params, timeout=30)\n",
    "                \n",
    "                if r.status_code == 429:\n",
    "                    print(f\"   ‚è≥ Rate limit hit. Waiting 60 seconds...\")\n",
    "                    time.sleep(60)\n",
    "                    continue\n",
    "                    \n",
    "                r.raise_for_status()\n",
    "                data = r.json()\n",
    "                \n",
    "                if data:\n",
    "                    all_trades.extend(data)\n",
    "                    print(f\"   ‚úÖ {current_date}: {len(data)} trades\")\n",
    "                else:\n",
    "                    print(f\"   ‚ö†Ô∏è  {current_date}: No trades\")\n",
    "                    \n",
    "                break\n",
    "                \n",
    "            except Exception as e:\n",
    "                if attempt == MAX_RETRIES - 1:\n",
    "                    print(f\"   ‚ùå Failed for {current_date}: {e}\")\n",
    "                print(f\"   ‚ö†Ô∏è  Attempt {attempt + 1} failed. Retrying...\")\n",
    "                time.sleep(5)\n",
    "        \n",
    "        current_date += timedelta(days=1)\n",
    "        time.sleep(1)  # Polite delay between days\n",
    "    \n",
    "    if not all_trades:\n",
    "        print(f\"   ‚ùå No trade data retrieved\")\n",
    "        return None\n",
    "    \n",
    "    # Process trades\n",
    "    df = pd.DataFrame(all_trades)\n",
    "    df['time_coinapi'] = pd.to_datetime(df['time_coinapi'])\n",
    "    df = df.rename(columns={'time_coinapi': 'timestamp'})\n",
    "    \n",
    "    # Aggregate to 5s intervals\n",
    "    df = df.set_index('timestamp')\n",
    "    agg = df.resample('5s').agg({\n",
    "        'price': 'mean',\n",
    "        'size': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    agg = agg.rename(columns={\n",
    "        'price': 'ETH_trade_price_mean',\n",
    "        'size': 'ETH_trade_volume_sum'\n",
    "    })\n",
    "    \n",
    "    print(f\"   ‚úÖ Aggregated to {len(agg)} 5s intervals\")\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6607b6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Helper: Fetch OHLCV Data from CoinAPI ----\n",
    "def fetch_ohlcv_data(asset_name, symbol_id, start_time, end_time, period='5SEC'):\n",
    "    \"\"\"\n",
    "    Fetch OHLCV data from CoinAPI for the given time range.\n",
    "    Returns DataFrame with [timestamp, price, volume].\n",
    "    \"\"\"\n",
    "    url = f\"https://rest.coinapi.io/v1/ohlcv/{symbol_id}/history\"\n",
    "    params = {\n",
    "        'period_id': period,\n",
    "        'time_start': start_time.isoformat() + 'Z',\n",
    "        'time_end': end_time.isoformat() + 'Z',\n",
    "        'limit': 10000  # CoinAPI max\n",
    "    }\n",
    "    \n",
    "    all_data = []\n",
    "    empty_count = 0\n",
    "    \n",
    "    # Fetch in chunks if needed\n",
    "    current_start = start_time\n",
    "    while current_start < end_time:\n",
    "        params['time_start'] = current_start.isoformat() + 'Z'\n",
    "        \n",
    "        for attempt in range(MAX_RETRIES):\n",
    "            try:\n",
    "                r = requests.get(url, headers=headers, params=params, timeout=30)\n",
    "                \n",
    "                if r.status_code == 429:\n",
    "                    print(f\"   ‚è≥ Rate limit hit. Waiting 60 seconds...\")\n",
    "                    time.sleep(60)\n",
    "                    continue\n",
    "                    \n",
    "                r.raise_for_status()\n",
    "                data = r.json()\n",
    "                \n",
    "                if len(data) == 0:\n",
    "                    empty_count += 1\n",
    "                    if empty_count >= NO_DATA_THRESHOLD:\n",
    "                        print(f\"   ‚ö†Ô∏è  No more data available for {asset_name}\")\n",
    "                        return pd.DataFrame(all_data) if all_data else pd.DataFrame()\n",
    "                    break\n",
    "                else:\n",
    "                    empty_count = 0\n",
    "                    all_data.extend(data)\n",
    "                    print(f\"   ‚úÖ Fetched {len(data)} rows for {asset_name} | Total: {len(all_data)}\")\n",
    "                    \n",
    "                    # Update start time for next chunk\n",
    "                    if len(data) > 0:\n",
    "                        last_time = pd.to_datetime(data[-1]['time_period_end'])\n",
    "                        current_start = last_time\n",
    "                    else:\n",
    "                        break\n",
    "                        \n",
    "                    if len(data) < 10000:  # Less than max, we're done\n",
    "                        return pd.DataFrame(all_data)\n",
    "                    break\n",
    "                    \n",
    "            except Exception as e:\n",
    "                if attempt == MAX_RETRIES - 1:\n",
    "                    print(f\"   ‚ùå Failed after {MAX_RETRIES} attempts: {e}\")\n",
    "                    return pd.DataFrame(all_data) if all_data else pd.DataFrame()\n",
    "                print(f\"   ‚ö†Ô∏è  Attempt {attempt + 1} failed: {e}. Retrying...\")\n",
    "                time.sleep(5)\n",
    "        \n",
    "        time.sleep(0.5)  # Polite delay\n",
    "    \n",
    "    return pd.DataFrame(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d0c23ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Helper: Process Asset Data ----\n",
    "def fetch_asset_data(asset_name, start_time, end_time):\n",
    "    \"\"\"Fetch and process asset data\"\"\"\n",
    "    symbol_id = ASSET_SYMBOLS.get(asset_name)\n",
    "    if not symbol_id:\n",
    "        print(f\"‚ùå No symbol mapping for {asset_name}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"üîÑ Fetching {asset_name} ({symbol_id})...\")\n",
    "    df = fetch_ohlcv_data(asset_name, symbol_id, start_time, end_time)\n",
    "    \n",
    "    if df.empty:\n",
    "        print(f\"   ‚ùå No data returned for {asset_name}\")\n",
    "        return None\n",
    "    \n",
    "    # Process data\n",
    "    df = df.rename(columns={\n",
    "        'time_period_start': 'timestamp',\n",
    "        'price_close': f'{asset_name}_price',\n",
    "        'volume_traded': f'{asset_name}_volume'\n",
    "    })\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df = df[['timestamp', f'{asset_name}_price', f'{asset_name}_volume']]\n",
    "    df = df.sort_values('timestamp').drop_duplicates('timestamp')\n",
    "    \n",
    "    print(f\"   ‚úÖ {asset_name}: {len(df)} rows\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfaf5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Fetch ETH Funding Rates ----\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéØ FETCHING ETH FUNDING RATES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    funding_df = fetch_eth_funding_rates(start_time, end_time)\n",
    "    \n",
    "    if funding_df is not None and not funding_df.empty:\n",
    "        print(\"üîÑ Merging funding rates with timeline...\")\n",
    "        merged = pd.merge_asof(\n",
    "            merged.sort_values('timestamp'),\n",
    "            funding_df.sort_values('timestamp'),\n",
    "            on='timestamp',\n",
    "            direction='backward'\n",
    "        )\n",
    "        print(f\"‚úÖ Funding rates merged: {funding_df.shape}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No funding rates data available (requires specialized subscription)\")\n",
    "        print(\"‚ÑπÔ∏è  Consider fetching directly from Binance/Bybit APIs if needed\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Funding rates fetch failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d7ee0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Fetch ETH Trades Data ----\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéØ FETCHING ETH TRADES DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    trades_df = fetch_eth_trades(start_time, end_time)\n",
    "    \n",
    "    if trades_df is not None and not trades_df.empty:\n",
    "        print(\"üîÑ Merging ETH trades with timeline...\")\n",
    "        merged = pd.merge_asof(\n",
    "            merged.sort_values('timestamp'),\n",
    "            trades_df.sort_values('timestamp'),\n",
    "            on='timestamp',\n",
    "            direction='backward',\n",
    "            tolerance=pd.Timedelta('5s')\n",
    "        )\n",
    "        print(f\"‚úÖ ETH trades merged: {trades_df.shape}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No ETH trades data available\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ETH trades fetch failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81bc82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Fetch all asset data ----\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéØ FETCHING ASSET DATA FROM COINAPI\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "merged = ref.copy()\n",
    "successful_assets = []\n",
    "failed_assets = []\n",
    "\n",
    "for asset in assets:\n",
    "    try:\n",
    "        asset_df = fetch_asset_data(asset, start_time, end_time)\n",
    "        \n",
    "        if asset_df is not None and not asset_df.empty:\n",
    "            # Merge with reference timeline\n",
    "            merged = pd.merge(merged, asset_df, on='timestamp', how='left')\n",
    "            successful_assets.append(asset)\n",
    "            print(f\"   ‚úÖ {asset} merged successfully\")\n",
    "        else:\n",
    "            failed_assets.append(asset)\n",
    "            print(f\"   ‚ùå {asset} failed - no data\")\n",
    "            \n",
    "        time.sleep(2)  # Polite delay between assets\n",
    "        \n",
    "    except Exception as e:\n",
    "        failed_assets.append(asset)\n",
    "        print(f\"   ‚ùå {asset} failed: {e}\")\n",
    "\n",
    "print(f\"\\nüìä Asset Fetch Results:\")\n",
    "print(f\"   ‚úÖ Successful: {len(successful_assets)}/{len(assets)}\")\n",
    "print(f\"   ‚ùå Failed: {len(failed_assets)}/{len(assets)}\")\n",
    "if failed_assets:\n",
    "    print(f\"   Failed assets: {', '.join(failed_assets)}\")\n",
    "\n",
    "print(f\"\\nüìä Merged shape: {merged.shape}\")\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cf21f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Load reference times from ETHUSD_5s.csv ----\n",
    "print(\"üìÇ Loading reference timeline from ETHUSD_5s.csv...\")\n",
    "ref = pd.read_csv(ref_file)\n",
    "\n",
    "# Find timestamp column\n",
    "time_col = None\n",
    "for possible_col in ['timestamp', 'time', 'datetime', 'Date']:\n",
    "    if possible_col in ref.columns:\n",
    "        time_col = possible_col\n",
    "        break\n",
    "\n",
    "if not time_col:\n",
    "    raise ValueError(f\"Could not find timestamp column in {ref_file}. Columns: {ref.columns.tolist()}\")\n",
    "\n",
    "ref[time_col] = pd.to_datetime(ref[time_col], utc=True)\n",
    "ref = ref.rename(columns={time_col: 'timestamp'})\n",
    "ref = ref[['timestamp']].sort_values('timestamp').drop_duplicates()\n",
    "\n",
    "print(f\"‚úÖ Reference timeline loaded\")\n",
    "print(f\"   Time range: {ref['timestamp'].min()} to {ref['timestamp'].max()}\")\n",
    "print(f\"   Total timestamps: {len(ref):,}\")\n",
    "print(f\"   Duration: {(ref['timestamp'].max() - ref['timestamp'].min()).days} days\")\n",
    "\n",
    "# Get time range for queries\n",
    "start_time = ref['timestamp'].min().to_pydatetime().replace(tzinfo=None)\n",
    "end_time = ref['timestamp'].max().to_pydatetime().replace(tzinfo=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44befce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Save final dataset ----\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üíæ SAVING FINAL DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create save directory if needed\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "merged.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"‚úÖ Dataset saved: {output_file}\")\n",
    "print(f\"üìä Final shape: {merged.shape}\")\n",
    "print(f\"üìÖ Date range: {merged['timestamp'].min()} to {merged['timestamp'].max()}\")\n",
    "print(f\"üìè Duration: {(merged['timestamp'].max() - merged['timestamp'].min()).days} days\")\n",
    "\n",
    "# Data quality report\n",
    "print(f\"\\nüìà DATA COVERAGE REPORT:\")\n",
    "print(f\"{'Column':<30} {'Non-Null':<12} {'Coverage %':<12}\")\n",
    "print(\"-\" * 54)\n",
    "\n",
    "for col in merged.columns:\n",
    "    if col != 'timestamp':\n",
    "        non_null = merged[col].notna().sum()\n",
    "        coverage = (non_null / len(merged)) * 100\n",
    "        print(f\"{col:<30} {non_null:<12,} {coverage:<11.2f}%\")\n",
    "\n",
    "file_size_mb = os.path.getsize(output_file) / (1024*1024)\n",
    "print(f\"\\nüíæ File size: {file_size_mb:.2f} MB\")\n",
    "print(f\"\\nüéâ Dataset ready for backtesting!\")\n",
    "\n",
    "merged.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
