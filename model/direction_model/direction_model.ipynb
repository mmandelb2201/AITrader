{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b290623",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ddaa1e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.18.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Dict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cf339d",
   "metadata": {},
   "source": [
    "## Hyperparameters - Easily Adjustable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "117d7b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lookback: 60, Horizon: 5\n",
      "Profit Target: 1.0%, Stop Loss: 1.0%\n",
      "Dynamic Barriers: True\n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "LOOKBACK_L = 60          # Window size for feature sequences\n",
    "HORIZON_H = 5           # Forecast horizon (steps ahead)\n",
    "\n",
    "# Triple barrier parameters\n",
    "PT_PCT = 0.01           # Profit target: 1.0% (or use dynamic based on volatility)\n",
    "SL_PCT = 0.010           # Stop loss: 1.0%\n",
    "USE_DYNAMIC_BARRIERS = True  # Use volatility-based barriers\n",
    "VOL_MULTIPLIER = 1.5     # Multiplier for volatility-based barriers\n",
    "\n",
    "# Training parameters\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "print(f\"Lookback: {LOOKBACK_L}, Horizon: {HORIZON_H}\")\n",
    "print(f\"Profit Target: {PT_PCT*100:.1f}%, Stop Loss: {SL_PCT*100:.1f}%\")\n",
    "print(f\"Dynamic Barriers: {USE_DYNAMIC_BARRIERS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f03fd0a",
   "metadata": {},
   "source": [
    "## Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d0169642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_merge_data(eth_path: str, btc_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load ETH and BTC data, merge on timestamp with forward fill.\n",
    "    \n",
    "    Args:\n",
    "        eth_path: Path to ETH CSV file\n",
    "        btc_path: Path to BTC CSV file\n",
    "    \n",
    "    Returns:\n",
    "        Merged DataFrame with ETH and BTC data\n",
    "    \"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    eth_df = pd.read_csv(eth_path)\n",
    "    btc_df = pd.read_csv(btc_path)\n",
    "    \n",
    "    # Rename 'time' column to 'timestamp' if it exists\n",
    "    if 'time' in eth_df.columns:\n",
    "        eth_df = eth_df.rename(columns={'time': 'timestamp'})\n",
    "    if 'time' in btc_df.columns:\n",
    "        btc_df = btc_df.rename(columns={'time': 'timestamp'})\n",
    "    \n",
    "    # Ensure timestamp columns\n",
    "    eth_df['timestamp'] = pd.to_datetime(eth_df['timestamp'])\n",
    "    btc_df['timestamp'] = pd.to_datetime(btc_df['timestamp'])\n",
    "    \n",
    "    # Merge on timestamp\n",
    "    df = pd.merge(eth_df, btc_df, on='timestamp', how='outer', suffixes=('_eth', '_btc'))\n",
    "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "    \n",
    "    # Rename close price columns for consistency\n",
    "    if 'ETH_close' in df.columns:\n",
    "        df = df.rename(columns={'ETH_close': 'price_eth'})\n",
    "    if 'BTC_close' in df.columns:\n",
    "        df = df.rename(columns={'BTC_close': 'price_btc'})\n",
    "    \n",
    "    # Rename volume columns if they exist\n",
    "    if 'ETH_volume' in df.columns:\n",
    "        df = df.rename(columns={'ETH_volume': 'volume_eth'})\n",
    "    if 'BTC_volume' in df.columns:\n",
    "        df = df.rename(columns={'BTC_volume': 'volume_btc'})\n",
    "    \n",
    "    # Forward fill missing values\n",
    "    df = df.fillna(method='ffill').dropna()\n",
    "    \n",
    "    print(f\"Merged data shape: {df.shape}\")\n",
    "    print(f\"Date range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "1c9e2210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Merged data shape: (1046782, 5)\n",
      "Date range: 2023-11-06 16:31:00+00:00 to 2025-11-10 16:24:00+00:00\n",
      "Merged data shape: (1046782, 5)\n",
      "Date range: 2023-11-06 16:31:00+00:00 to 2025-11-10 16:24:00+00:00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>price_eth</th>\n",
       "      <th>volume_eth</th>\n",
       "      <th>price_btc</th>\n",
       "      <th>volume_btc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-11-06 16:31:00+00:00</td>\n",
       "      <td>1898.02</td>\n",
       "      <td>41.677949</td>\n",
       "      <td>35059.11</td>\n",
       "      <td>2.883760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-11-06 16:32:00+00:00</td>\n",
       "      <td>1897.83</td>\n",
       "      <td>43.216107</td>\n",
       "      <td>35056.92</td>\n",
       "      <td>4.001168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-11-06 16:33:00+00:00</td>\n",
       "      <td>1898.02</td>\n",
       "      <td>69.148875</td>\n",
       "      <td>35055.26</td>\n",
       "      <td>2.488042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-11-06 16:34:00+00:00</td>\n",
       "      <td>1897.29</td>\n",
       "      <td>83.485523</td>\n",
       "      <td>35050.93</td>\n",
       "      <td>9.153628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023-11-06 16:35:00+00:00</td>\n",
       "      <td>1899.25</td>\n",
       "      <td>130.998962</td>\n",
       "      <td>35082.45</td>\n",
       "      <td>3.128783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  timestamp  price_eth  volume_eth  price_btc  volume_btc\n",
       "3 2023-11-06 16:31:00+00:00    1898.02   41.677949   35059.11    2.883760\n",
       "4 2023-11-06 16:32:00+00:00    1897.83   43.216107   35056.92    4.001168\n",
       "5 2023-11-06 16:33:00+00:00    1898.02   69.148875   35055.26    2.488042\n",
       "6 2023-11-06 16:34:00+00:00    1897.29   83.485523   35050.93    9.153628\n",
       "7 2023-11-06 16:35:00+00:00    1899.25  130.998962   35082.45    3.128783"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "ETH_PATH = '../data/specific_asset_data/ETH_data.csv'\n",
    "BTC_PATH = '../data/specific_asset_data/BTC_data.csv'\n",
    "\n",
    "df = load_and_merge_data(ETH_PATH, BTC_PATH)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff632035",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "45b925a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create robust features for modeling:\n",
    "    - Log returns for ETH and BTC\n",
    "    - Rolling volatility (std of returns)\n",
    "    - Volume change percentage (capped to handle extreme values)\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame with price and volume columns\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with engineered features\n",
    "    \"\"\"\n",
    "    print(\"\\nCreating features...\")\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Log returns\n",
    "    df['eth_log_return'] = np.log(df['price_eth'] / df['price_eth'].shift(1))\n",
    "    df['btc_log_return'] = np.log(df['price_btc'] / df['price_btc'].shift(1))\n",
    "    \n",
    "    # Rolling volatility (10-period standard deviation of returns)\n",
    "    df['eth_volatility'] = df['eth_log_return'].rolling(window=10).std()\n",
    "    df['btc_volatility'] = df['btc_log_return'].rolling(window=10).std()\n",
    "    \n",
    "    # Volume change (with safeguards for extreme values)\n",
    "    # Use robust method: log of ratio instead of pct_change to avoid infinities\n",
    "    df['eth_volume_change'] = np.log(df['volume_eth'] / df['volume_eth'].shift(1).replace(0, 1e-10))\n",
    "    df['btc_volume_change'] = np.log(df['volume_btc'] / df['volume_btc'].shift(1).replace(0, 1e-10))\n",
    "    \n",
    "    # Cap extreme log volume changes at +/- 2 (roughly 640% up or 86% down)\n",
    "    df['eth_volume_change'] = df['eth_volume_change'].clip(-2, 2)\n",
    "    df['btc_volume_change'] = df['btc_volume_change'].clip(-2, 2)\n",
    "    \n",
    "    # Drop NaN rows from feature creation\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Features created. Shape after cleaning: {df.shape}\")\n",
    "    print(f\"Volume change range: ETH [{df['eth_volume_change'].min():.2f}, {df['eth_volume_change'].max():.2f}], \"\n",
    "          f\"BTC [{df['btc_volume_change'].min():.2f}, {df['btc_volume_change'].max():.2f}]\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "108a65e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating features...\n",
      "Features created. Shape after cleaning: (1046772, 11)\n",
      "Volume change range: ETH [-2.00, 2.00], BTC [-2.00, 2.00]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>price_eth</th>\n",
       "      <th>eth_log_return</th>\n",
       "      <th>eth_volatility</th>\n",
       "      <th>eth_volume_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-11-06 16:41:00+00:00</td>\n",
       "      <td>1899.10</td>\n",
       "      <td>-0.000216</td>\n",
       "      <td>0.000640</td>\n",
       "      <td>-0.625194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-11-06 16:42:00+00:00</td>\n",
       "      <td>1899.71</td>\n",
       "      <td>0.000321</td>\n",
       "      <td>0.000643</td>\n",
       "      <td>0.012527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-11-06 16:43:00+00:00</td>\n",
       "      <td>1899.82</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000643</td>\n",
       "      <td>0.438088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-11-06 16:44:00+00:00</td>\n",
       "      <td>1899.35</td>\n",
       "      <td>-0.000247</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>-0.698388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-11-06 16:45:00+00:00</td>\n",
       "      <td>1899.03</td>\n",
       "      <td>-0.000168</td>\n",
       "      <td>0.000546</td>\n",
       "      <td>0.447734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  timestamp  price_eth  eth_log_return  eth_volatility  \\\n",
       "0 2023-11-06 16:41:00+00:00    1899.10       -0.000216        0.000640   \n",
       "1 2023-11-06 16:42:00+00:00    1899.71        0.000321        0.000643   \n",
       "2 2023-11-06 16:43:00+00:00    1899.82        0.000058        0.000643   \n",
       "3 2023-11-06 16:44:00+00:00    1899.35       -0.000247        0.000633   \n",
       "4 2023-11-06 16:45:00+00:00    1899.03       -0.000168        0.000546   \n",
       "\n",
       "   eth_volume_change  \n",
       "0          -0.625194  \n",
       "1           0.012527  \n",
       "2           0.438088  \n",
       "3          -0.698388  \n",
       "4           0.447734  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create features\n",
    "df = create_features(df)\n",
    "df[['timestamp', 'price_eth', 'eth_log_return', 'eth_volatility', 'eth_volume_change']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8842f1a9",
   "metadata": {},
   "source": [
    "## Triple Barrier Labeling\n",
    "\n",
    "This is the core logic for generating directional labels:\n",
    "\n",
    "For each timestamp `t`:\n",
    "1. Set **vertical barrier** at `t + HORIZON_H` (time limit)\n",
    "2. Set **top barrier** (profit) at `current_price * (1 + PT)`\n",
    "3. Set **bottom barrier** (stop) at `current_price * (1 - SL)`\n",
    "4. Check which barrier is hit first:\n",
    "   - Top hit first → **Label = 1** (Buy signal)\n",
    "   - Bottom hit first → **Label = -1** (Sell/Avoid signal)\n",
    "   - Vertical hit first → **Label = 0** (Neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f5ba6b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_triple_barrier_labels(\n",
    "    df: pd.DataFrame,\n",
    "    price_col: str = 'price_eth',\n",
    "    horizon: int = HORIZON_H,\n",
    "    pt_pct: float = PT_PCT,\n",
    "    sl_pct: float = SL_PCT,\n",
    "    use_dynamic: bool = USE_DYNAMIC_BARRIERS,\n",
    "    vol_col: str = 'eth_volatility',\n",
    "    vol_mult: float = VOL_MULTIPLIER\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Implement Triple Barrier Labeling for directional prediction.\n",
    "    \n",
    "    For each timestamp t:\n",
    "    1. Set vertical barrier at t + horizon\n",
    "    2. Set top barrier (profit) at price * (1 + PT)\n",
    "    3. Set bottom barrier (stop) at price * (1 - SL)\n",
    "    4. Check which barrier is hit first:\n",
    "       - Top hit first → Label = 1 (Buy signal)\n",
    "       - Bottom hit first → Label = -1 (Sell/Avoid signal)\n",
    "       - Vertical hit first (time expires) → Label = 0 (Neutral)\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with price data\n",
    "        price_col: Column name for price\n",
    "        horizon: Forecast horizon (vertical barrier)\n",
    "        pt_pct: Profit target percentage (fixed)\n",
    "        sl_pct: Stop loss percentage (fixed)\n",
    "        use_dynamic: Use volatility-based dynamic barriers\n",
    "        vol_col: Volatility column for dynamic barriers\n",
    "        vol_mult: Multiplier for volatility\n",
    "    \n",
    "    Returns:\n",
    "        Array of labels (-1, 0, 1)\n",
    "    \"\"\"\n",
    "    print(\"\\nGenerating Triple Barrier Labels...\")\n",
    "    print(f\"  Horizon: {horizon} steps\")\n",
    "    print(f\"  Profit Target: {pt_pct*100:.2f}%\")\n",
    "    print(f\"  Stop Loss: {sl_pct*100:.2f}%\")\n",
    "    print(f\"  Dynamic Barriers: {use_dynamic}\")\n",
    "    \n",
    "    prices = df[price_col].values\n",
    "    n = len(prices)\n",
    "    labels = np.zeros(n, dtype=int)\n",
    "    \n",
    "    # Get volatility if using dynamic barriers\n",
    "    if use_dynamic and vol_col in df.columns:\n",
    "        volatility = df[vol_col].values\n",
    "    else:\n",
    "        volatility = None\n",
    "    \n",
    "    for i in range(n - horizon):\n",
    "        current_price = prices[i]\n",
    "        \n",
    "        # Determine barrier widths\n",
    "        if use_dynamic and volatility is not None:\n",
    "            vol = volatility[i] if not np.isnan(volatility[i]) else pt_pct\n",
    "            pt = vol * vol_mult\n",
    "            sl = vol * vol_mult * 0.7  # Asymmetric: tighter stop\n",
    "        else:\n",
    "            pt = pt_pct\n",
    "            sl = sl_pct\n",
    "        \n",
    "        # Set barriers\n",
    "        top_barrier = current_price * (1 + pt)\n",
    "        bottom_barrier = current_price * (1 - sl)\n",
    "        \n",
    "        # Look ahead over the horizon\n",
    "        future_prices = prices[i+1:i+1+horizon]\n",
    "        \n",
    "        # Find first barrier touch\n",
    "        hit_top = False\n",
    "        hit_bottom = False\n",
    "        \n",
    "        for future_price in future_prices:\n",
    "            if future_price >= top_barrier:\n",
    "                hit_top = True\n",
    "                break\n",
    "            elif future_price <= bottom_barrier:\n",
    "                hit_bottom = True\n",
    "                break\n",
    "        \n",
    "        # Assign label\n",
    "        if hit_top:\n",
    "            labels[i] = 1   # Buy signal (profit target hit)\n",
    "        elif hit_bottom:\n",
    "            labels[i] = -1  # Sell/Avoid signal (stop loss hit)\n",
    "        else:\n",
    "            labels[i] = 0   # Neutral (time barrier hit)\n",
    "    \n",
    "    # Last `horizon` samples cannot be labeled → assign neutral\n",
    "    labels[-horizon:] = 0\n",
    "    \n",
    "    # Print label distribution\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    print(\"\\nLabel Distribution:\")\n",
    "    for label, count in zip(unique, counts):\n",
    "        label_name = {-1: \"Sell/Avoid\", 0: \"Neutral\", 1: \"Buy\"}[label]\n",
    "        print(f\"  {label_name:12} ({label:2d}): {count:6d} ({count/len(labels)*100:5.2f}%)\")\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a750fb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Triple Barrier Labels...\n",
      "  Horizon: 5 steps\n",
      "  Profit Target: 1.00%\n",
      "  Stop Loss: 1.00%\n",
      "  Dynamic Barriers: True\n",
      "\n",
      "Label Distribution:\n",
      "  Sell/Avoid   (-1): 464915 (44.41%)\n",
      "  Neutral      ( 0): 236785 (22.62%)\n",
      "  Buy          ( 1): 345072 (32.97%)\n",
      "\n",
      "Label Distribution:\n",
      "  Sell/Avoid   (-1): 464915 (44.41%)\n",
      "  Neutral      ( 0): 236785 (22.62%)\n",
      "  Buy          ( 1): 345072 (32.97%)\n"
     ]
    }
   ],
   "source": [
    "# Generate labels\n",
    "labels = get_triple_barrier_labels(\n",
    "    df,\n",
    "    price_col='price_eth',\n",
    "    horizon=HORIZON_H,\n",
    "    pt_pct=PT_PCT,\n",
    "    sl_pct=SL_PCT,\n",
    "    use_dynamic=USE_DYNAMIC_BARRIERS,\n",
    "    vol_col='eth_volatility',\n",
    "    vol_mult=VOL_MULTIPLIER\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5257feb7",
   "metadata": {},
   "source": [
    "## Create Sequences for Time-Series Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "60fc983f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(\n",
    "    df: pd.DataFrame,\n",
    "    labels: np.ndarray,\n",
    "    feature_cols: list,\n",
    "    lookback: int = LOOKBACK_L\n",
    ") -> Tuple[np.ndarray, np.ndarray, StandardScaler]:\n",
    "    \"\"\"\n",
    "    Create sequences for time-series modeling.\n",
    "    Filters out neutral labels (0) and keeps only directional signals.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with features\n",
    "        labels: Array of labels (-1, 0, 1)\n",
    "        feature_cols: List of feature column names\n",
    "        lookback: Sequence length\n",
    "    \n",
    "    Returns:\n",
    "        X: Feature sequences (samples, lookback, features)\n",
    "        y: Binary labels (samples,) - 0 for down, 1 for up\n",
    "        scaler: Fitted StandardScaler\n",
    "    \"\"\"\n",
    "    print(f\"\\nCreating sequences with lookback={lookback}...\")\n",
    "    \n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    features = df[feature_cols].values\n",
    "    features_scaled = scaler.fit_transform(features)\n",
    "    \n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(lookback, len(features)):\n",
    "        # Skip neutral labels (0)\n",
    "        if labels[i] != 0:\n",
    "            X.append(features_scaled[i-lookback:i])\n",
    "            y.append(labels[i])\n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # Convert labels to binary: -1→0 (down), 1→1 (up)\n",
    "    y = (y + 1) // 2\n",
    "    \n",
    "    print(f\"Before balancing: Down (0): {(y==0).sum()}, Up (1): {(y==1).sum()}\")\n",
    "    \n",
    "    # Balance classes by downsampling the majority class\n",
    "    down_indices = np.where(y == 0)[0]\n",
    "    up_indices = np.where(y == 1)[0]\n",
    "    \n",
    "    # Downsample to match minority class\n",
    "    min_samples = min(len(down_indices), len(up_indices))\n",
    "    \n",
    "    if len(down_indices) > min_samples:\n",
    "        down_indices = np.random.choice(down_indices, min_samples, replace=False)\n",
    "    if len(up_indices) > min_samples:\n",
    "        up_indices = np.random.choice(up_indices, min_samples, replace=False)\n",
    "    \n",
    "    # Combine and shuffle\n",
    "    balanced_indices = np.concatenate([down_indices, up_indices])\n",
    "    np.random.shuffle(balanced_indices)\n",
    "    \n",
    "    X = X[balanced_indices]\n",
    "    y = y[balanced_indices]\n",
    "\n",
    "    print(f\"Sequences created: X shape = {X.shape}, y shape = {y.shape}\")\n",
    "    print(f\"After balancing: Down (0): {(y==0).sum()}, Up (1): {(y==1).sum()}\")\n",
    "    \n",
    "    return X, y, scaler    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "83e6995b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating sequences with lookback=60...\n",
      "Before balancing: Down (0): 464886, Up (1): 345058\n",
      "Before balancing: Down (0): 464886, Up (1): 345058\n",
      "Sequences created: X shape = (690116, 60, 6), y shape = (690116,)\n",
      "After balancing: Down (0): 345058, Up (1): 345058\n",
      "\n",
      "Feature sequence shape: (690116, 60, 6)\n",
      "Labels shape: (690116,)\n",
      "Number of features: 6\n",
      "Sequences created: X shape = (690116, 60, 6), y shape = (690116,)\n",
      "After balancing: Down (0): 345058, Up (1): 345058\n",
      "\n",
      "Feature sequence shape: (690116, 60, 6)\n",
      "Labels shape: (690116,)\n",
      "Number of features: 6\n"
     ]
    }
   ],
   "source": [
    "# Define feature columns\n",
    "feature_cols = [\n",
    "    'eth_log_return', 'btc_log_return',\n",
    "    'eth_volatility', 'btc_volatility',\n",
    "    'eth_volume_change', 'btc_volume_change'\n",
    "]\n",
    "\n",
    "# Create sequences\n",
    "X, y, scaler = create_sequences(df, labels, feature_cols, lookback=LOOKBACK_L)\n",
    "\n",
    "print(f\"\\nFeature sequence shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "print(f\"Number of features: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03115e1",
   "metadata": {},
   "source": [
    "## Train/Validation/Test Split (Chronological)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "86b2ac1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    train_size: float = 0.7,\n",
    "    val_size: float = 0.15\n",
    ") -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Split data chronologically (no shuffling for time-series).\n",
    "    \n",
    "    Args:\n",
    "        X: Feature sequences\n",
    "        y: Labels\n",
    "        train_size: Proportion for training\n",
    "        val_size: Proportion for validation\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with train/val/test splits\n",
    "    \"\"\"\n",
    "    n = len(X)\n",
    "    train_end = int(n * train_size)\n",
    "    val_end = int(n * (train_size + val_size))\n",
    "    \n",
    "    splits = {\n",
    "        'X_train': X[:train_end],\n",
    "        'y_train': y[:train_end],\n",
    "        'X_val': X[train_end:val_end],\n",
    "        'y_val': y[train_end:val_end],\n",
    "        'X_test': X[val_end:],\n",
    "        'y_test': y[val_end:]\n",
    "    }\n",
    "    \n",
    "    print(\"\\nData split (chronological):\")\n",
    "    print(f\"  Train: {len(splits['X_train']):6d} samples ({train_size*100:.0f}%)\")\n",
    "    print(f\"  Val:   {len(splits['X_val']):6d} samples ({val_size*100:.0f}%)\")\n",
    "    print(f\"  Test:  {len(splits['X_test']):6d} samples ({(1-train_size-val_size)*100:.0f}%)\")\n",
    "    \n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a1fe888b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data split (chronological):\n",
      "  Train: 483081 samples (70%)\n",
      "  Val:   103517 samples (15%)\n",
      "  Test:  103518 samples (15%)\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "splits = split_data(X, y, train_size=0.7, val_size=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49159b7b",
   "metadata": {},
   "source": [
    "## Build CNN-LSTM Model\n",
    "\n",
    "Architecture:\n",
    "- **Conv1D layers**: Extract local patterns from sequences\n",
    "- **MaxPooling1D**: Downsample features\n",
    "- **LSTM layers**: Capture long-term temporal dependencies\n",
    "- **Dense layers**: Classification head with softmax (3 classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "507261b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_lstm_model(\n",
    "    input_shape: Tuple[int, int],\n",
    "    num_classes: int = 2\n",
    ") -> keras.Model:\n",
    "    \"\"\"\n",
    "    Build CNN-LSTM model for binary directional prediction.\n",
    "    \n",
    "    Architecture:\n",
    "    - Conv1D: Extract local patterns\n",
    "    - MaxPooling1D: Downsample\n",
    "    - LSTM: Capture temporal dependencies\n",
    "    - Dense: Classification head\n",
    "    \n",
    "    Args:\n",
    "        input_shape: (lookback, n_features)\n",
    "        num_classes: Number of output classes (2: Down/Up)\n",
    "    \n",
    "    Returns:\n",
    "        Compiled Keras model\n",
    "    \"\"\"\n",
    "    print(\"\\nBuilding CNN-LSTM model...\")\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        # Input layer\n",
    "        layers.Input(shape=input_shape),\n",
    "        \n",
    "        # CNN layers for local feature extraction\n",
    "        layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        \n",
    "        layers.Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        \n",
    "        # Single LSTM layer for temporal dependencies\n",
    "        layers.LSTM(units=100),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Classification head\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "98c9effe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building CNN-LSTM model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,216</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_10          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_11          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">91,600</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,464</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_10 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │         \u001b[38;5;34m1,216\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_10          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_10 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_11 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m24,704\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_11          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_11 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_7 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m91,600\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_12 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m6,464\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_13 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m130\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">124,882</span> (487.82 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m124,882\u001b[0m (487.82 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">124,498</span> (486.32 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m124,498\u001b[0m (486.32 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> (1.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m384\u001b[0m (1.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build model\n",
    "model = build_cnn_lstm_model(input_shape=(LOOKBACK_L, len(feature_cols)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da06cdd7",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "360b6085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model...\n",
      "Batch size: 64, Max epochs: 50\n"
     ]
    }
   ],
   "source": [
    "# Setup callbacks\n",
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "print(\"\\nTraining model...\")\n",
    "print(f\"Batch size: {BATCH_SIZE}, Max epochs: {EPOCHS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "fdfff157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m7549/7549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 19ms/step - accuracy: 0.5054 - loss: 0.6951 - val_accuracy: 0.5034 - val_loss: 0.6931 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m7549/7549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 19ms/step - accuracy: 0.5054 - loss: 0.6951 - val_accuracy: 0.5034 - val_loss: 0.6931 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m7549/7549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 18ms/step - accuracy: 0.5127 - loss: 0.6927 - val_accuracy: 0.5167 - val_loss: 0.6923 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m7549/7549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 18ms/step - accuracy: 0.5127 - loss: 0.6927 - val_accuracy: 0.5167 - val_loss: 0.6923 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m7549/7549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 18ms/step - accuracy: 0.5133 - loss: 0.6926 - val_accuracy: 0.5186 - val_loss: 0.6920 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m7549/7549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 18ms/step - accuracy: 0.5133 - loss: 0.6926 - val_accuracy: 0.5186 - val_loss: 0.6920 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m7549/7549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 18ms/step - accuracy: 0.5144 - loss: 0.6925 - val_accuracy: 0.5137 - val_loss: 0.6923 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m7549/7549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 18ms/step - accuracy: 0.5144 - loss: 0.6925 - val_accuracy: 0.5137 - val_loss: 0.6923 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m7549/7549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 18ms/step - accuracy: 0.5191 - loss: 0.6922 - val_accuracy: 0.5149 - val_loss: 0.6922 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m7549/7549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 18ms/step - accuracy: 0.5191 - loss: 0.6922 - val_accuracy: 0.5149 - val_loss: 0.6922 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m7549/7549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 18ms/step - accuracy: 0.5181 - loss: 0.6921 - val_accuracy: 0.5213 - val_loss: 0.6919 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m7549/7549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 18ms/step - accuracy: 0.5181 - loss: 0.6921 - val_accuracy: 0.5213 - val_loss: 0.6919 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m7549/7549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 19ms/step - accuracy: 0.5223 - loss: 0.6918 - val_accuracy: 0.5181 - val_loss: 0.6920 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m7549/7549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 19ms/step - accuracy: 0.5223 - loss: 0.6918 - val_accuracy: 0.5181 - val_loss: 0.6920 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m7549/7549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 20ms/step - accuracy: 0.5235 - loss: 0.6915 - val_accuracy: 0.5228 - val_loss: 0.6917 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m7549/7549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 20ms/step - accuracy: 0.5235 - loss: 0.6915 - val_accuracy: 0.5228 - val_loss: 0.6917 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m7549/7549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8013s\u001b[0m 1s/step - accuracy: 0.5255 - loss: 0.6910 - val_accuracy: 0.5191 - val_loss: 0.6918 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m7549/7549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8013s\u001b[0m 1s/step - accuracy: 0.5255 - loss: 0.6910 - val_accuracy: 0.5191 - val_loss: 0.6918 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m7549/7549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5581s\u001b[0m 739ms/step - accuracy: 0.5296 - loss: 0.6905 - val_accuracy: 0.5215 - val_loss: 0.6917 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m7549/7549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5581s\u001b[0m 739ms/step - accuracy: 0.5296 - loss: 0.6905 - val_accuracy: 0.5215 - val_loss: 0.6917 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m7549/7549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21057s\u001b[0m 3s/step - accuracy: 0.5317 - loss: 0.6899 - val_accuracy: 0.5231 - val_loss: 0.6917 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m7549/7549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21057s\u001b[0m 3s/step - accuracy: 0.5317 - loss: 0.6899 - val_accuracy: 0.5231 - val_loss: 0.6917 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m7549/7549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17616s\u001b[0m 2s/step - accuracy: 0.5368 - loss: 0.6886 - val_accuracy: 0.5186 - val_loss: 0.6920 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m7549/7549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17616s\u001b[0m 2s/step - accuracy: 0.5368 - loss: 0.6886 - val_accuracy: 0.5186 - val_loss: 0.6920 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m7549/7549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18254s\u001b[0m 2s/step - accuracy: 0.5394 - loss: 0.6879 - val_accuracy: 0.5222 - val_loss: 0.6930 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m7549/7549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18254s\u001b[0m 2s/step - accuracy: 0.5394 - loss: 0.6879 - val_accuracy: 0.5222 - val_loss: 0.6930 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m7549/7549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m245s\u001b[0m 32ms/step - accuracy: 0.5496 - loss: 0.6840 - val_accuracy: 0.5242 - val_loss: 0.6936 - learning_rate: 5.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m7549/7549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m245s\u001b[0m 32ms/step - accuracy: 0.5496 - loss: 0.6840 - val_accuracy: 0.5242 - val_loss: 0.6936 - learning_rate: 5.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m7549/7549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 17ms/step - accuracy: 0.5573 - loss: 0.6804 - val_accuracy: 0.5251 - val_loss: 0.6937 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m7549/7549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 17ms/step - accuracy: 0.5573 - loss: 0.6804 - val_accuracy: 0.5251 - val_loss: 0.6937 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m7549/7549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 17ms/step - accuracy: 0.5663 - loss: 0.6768 - val_accuracy: 0.5271 - val_loss: 0.6941 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m7549/7549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 17ms/step - accuracy: 0.5663 - loss: 0.6768 - val_accuracy: 0.5271 - val_loss: 0.6941 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m3739/7549\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:01\u001b[0m 16ms/step - accuracy: 0.5712 - loss: 0.6739"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[142], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplits\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mX_train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplits\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my_train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplits\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mX_val\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplits\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my_val\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:371\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    370\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 371\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:219\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    217\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[1;32m    218\u001b[0m     ):\n\u001b[0;32m--> 219\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[1;32m    221\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tensorflow/python/eager/context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1698\u001b[0m   )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train\n",
    "history = model.fit(\n",
    "    splits['X_train'], splits['y_train'],\n",
    "    validation_data=(splits['X_val'], splits['y_val']),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14f09d2",
   "metadata": {},
   "source": [
    "## Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df27d1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "ax1.plot(history.history['loss'], label='Train Loss')\n",
    "ax1.plot(history.history['val_loss'], label='Val Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training & Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy\n",
    "ax2.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "ax2.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Training & Validation Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4d2ee7",
   "metadata": {},
   "source": [
    "## Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec36612",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EVALUATION ON TEST SET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Predictions\n",
    "y_pred_probs = model.predict(splits['X_test'])\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "y_true = splits['y_test']\n",
    "\n",
    "# Accuracy\n",
    "accuracy = np.mean(y_pred == y_true)\n",
    "print(f\"\\nTest Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea1be5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class metrics\n",
    "print(\"\\nPer-Class Performance:\")\n",
    "class_names = ['Down/Sell', 'Up/Buy']\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    mask_true = (y_true == i)\n",
    "    mask_pred = (y_pred == i)\n",
    "    \n",
    "    true_positives = np.sum((y_true == i) & (y_pred == i))\n",
    "    precision = true_positives / np.sum(mask_pred) if np.sum(mask_pred) > 0 else 0\n",
    "    recall = true_positives / np.sum(mask_true) if np.sum(mask_true) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    print(f\"  {class_name:12}: Precision={precision:.3f}, Recall={recall:.3f}, F1={f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ad7bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(\"              Predicted\")\n",
    "print(\"              Sell  Neut  Buy\")\n",
    "for i, row in enumerate(cm):\n",
    "    print(f\"  Actual {class_names[i]:7}: {row}\")\n",
    "\n",
    "# Visualize confusion matrix\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75363718",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293b6e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save('eth_direction_model.h5')\n",
    "print(\"\\n✓ Model saved as 'eth_direction_model.h5'\")\n",
    "\n",
    "# Save scaler\n",
    "import joblib\n",
    "joblib.dump(scaler, 'feature_scaler.pkl')\n",
    "print(\"✓ Scaler saved as 'feature_scaler.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e0c13a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implements a complete directional prediction pipeline:\n",
    "\n",
    "1. **Triple Barrier Labeling**: Novel labeling method using profit target, stop loss, and time barriers\n",
    "2. **Feature Engineering**: Log returns, volatility, and volume changes for ETH and BTC\n",
    "3. **CNN-LSTM Architecture**: Combines local pattern extraction with temporal modeling\n",
    "4. **Proper Time-Series Handling**: Chronological splits, no data leakage\n",
    "\n",
    "### Key Hyperparameters:\n",
    "- `LOOKBACK_L`: Sequence length for features\n",
    "- `HORIZON_H`: Forecast horizon for labels\n",
    "- `PT_PCT`, `SL_PCT`: Profit/Stop thresholds\n",
    "- `USE_DYNAMIC_BARRIERS`: Volatility-based adaptive barriers\n",
    "\n",
    "### Next Steps:\n",
    "- Tune hyperparameters using grid search\n",
    "- Add more features (technical indicators, sentiment, etc.)\n",
    "- Implement backtesting with the predictions\n",
    "- Deploy for live trading"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
